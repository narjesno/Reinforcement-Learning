{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CCRUSi0rfBAc"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "!pip install import-ipynb\n",
    "import import_ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0z4ehxH3zN4W"
   },
   "outputs": [],
   "source": [
    "%cd drive\n",
    "%cd 'My Drive'\n",
    "%cd 'RL'\n",
    "%cd 'Homework'\n",
    "%cd 'Homework 4'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MaRbn7L5z0rI"
   },
   "outputs": [],
   "source": [
    "'''Needed Libraries and Environment'''\n",
    "import Environment\n",
    "import Modified_Environment\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import math\n",
    "import random\n",
    "from collections import defaultdict\n",
    "import sys\n",
    "import itertools\n",
    "import matplotlib\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "'''Constants Defined'''\n",
    "PROBABILITY = 0\n",
    "STATE = 1\n",
    "ACTION = 2\n",
    "REWARD = 3\n",
    "DUMMY = None\n",
    "MAX_T = 10000\n",
    "TOTAL_EPISODES = 2000\n",
    "gamma = 0.9\n",
    "ALPHA = 0.2\n",
    "MIN = 0.1\n",
    "DECAY_FACTOR = 0.99\n",
    "n = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "executionInfo": {
     "elapsed": 1297,
     "status": "ok",
     "timestamp": 1609760581450,
     "user": {
      "displayName": "Narjes Noorzad",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gj7tkVNhkAPpQLlGPleUdGAeLotodizEcuFCkFIkA=s64",
      "userId": "16921968874120068723"
     },
     "user_tz": -210
    },
    "id": "StR7y4zseEJA"
   },
   "outputs": [],
   "source": [
    "class TreeBackupAgent():\n",
    "  \n",
    "    def __init__(self, env, gamma):\n",
    "      self.env = env\n",
    "      self.Q = defaultdict(lambda: np.zeros(env.action_space.n))\n",
    "      self.actions = env.action_space.n\n",
    "      self.gamma = gamma\n",
    "      self.TB_episode_scores = []\n",
    "\n",
    "    def epsilon_greedy_policy(self, state, epsilon):\n",
    "        epsilon_policy = np.ones(self.actions, dtype=float) * epsilon / self.actions\n",
    "        best_action = np.argmax(self.Q[state])\n",
    "        epsilon_policy[best_action] += (1.0 - epsilon)\n",
    "        return epsilon_policy\n",
    "\n",
    "\n",
    "    def reach_island(self):\n",
    "        epsilon = 1 #<-- uncomment if you want to use decaying epsilon \n",
    "        #epsilon = 0.3 #<-- uncomment if you want to use constant epsilon\n",
    "        for episode in range(1, TOTAL_EPISODES + 1):\n",
    "            episode_score = 0\n",
    "            if episode % 2 == 0:\n",
    "                print(\"\\rEpisode {}/{}.\".format(episode, TOTAL_EPISODES), end=\"\")\n",
    "                sys.stdout.flush()\n",
    "                epsilon = max(epsilon * DECAY_FACTOR, MIN)#<-- uncomment if you want to use decaying epsilon\n",
    "            state = self.env.reset()\n",
    "            done = False\n",
    "            for t in itertools.count():\n",
    "                #first_step\n",
    "                probs = self.epsilon_greedy_policy(state, epsilon)\n",
    "                action = np.random.choice(np.arange(len(probs)), p = probs)\n",
    "                prime_state, reward, _, _ = self.env.step(action)\n",
    "                \n",
    "                prime_probs = self.epsilon_greedy_policy(prime_state, epsilon)\n",
    "                prime_action = np.random.choice(np.arange(len(prime_probs)), p = prime_probs )\n",
    "                double_prime_state, prime_reward, _, _ = env.step(prime_action)\n",
    "                #second_step\n",
    "                double_prime_probs = self.epsilon_greedy_policy(double_prime_state, epsilon)\n",
    "                double_prime_action = np.random.choice(np.arange(len(double_prime_probs)), p = double_prime_probs)\n",
    "                triple_prime_state, double_prime_reward, done, _ = env.step(double_prime_action)\n",
    "                #third_step\n",
    "                triple_prime_probs  = self.epsilon_greedy_policy(triple_prime_state, epsilon)\n",
    "                triple_prime_action = np.random.choice(np.arange(len(triple_prime_probs)), p = triple_prime_probs)\n",
    "                information = [probs, state, action, reward]\n",
    "                prime_information = [prime_probs, prime_state, prime_action, prime_reward]\n",
    "                double_prime_information = [double_prime_probs, double_prime_state, double_prime_action, double_prime_reward]\n",
    "                triple_prime_information = [triple_prime_probs, triple_prime_state]\n",
    "\n",
    "                self.TB_returns(information, prime_information, double_prime_information, triple_prime_information)\n",
    "\n",
    "                if done or t > MAX_T: break\n",
    "                state = prime_state\n",
    "                episode_score += reward\n",
    "            self.TB_episode_scores.append(episode_score)\n",
    "\n",
    "    def TB_returns(self, info, prime_info, double_prime_info, triple_prime_info):\n",
    "\n",
    "        V = np.sum(prime_info[PROBABILITY] *self.Q[prime_info[STATE]])\n",
    "        first_step = info[REWARD] + self.gamma * V\n",
    "\n",
    "        prime_V = np.sum(double_prime_info[PROBABILITY] *self.Q[double_prime_info[STATE]])            \n",
    "        first_step_error = prime_info[REWARD] + self.gamma * prime_V - self.Q[prime_info[STATE]][prime_info[ACTION]]\n",
    "        prime_action_selection_prob = max(prime_info[PROBABILITY])            \n",
    "\n",
    "        second_step = self.gamma * prime_action_selection_prob * first_step_error\n",
    "\n",
    "        double_prime_V = np.sum(triple_prime_info[PROBABILITY] *self.Q[triple_prime_info[STATE]])\n",
    "        second_step_error = double_prime_info[REWARD] + self.gamma * double_prime_V - self.Q[double_prime_info[STATE]][double_prime_info[ACTION]]\n",
    "        double_prime_action_selection_prob = max(double_prime_info[PROBABILITY])\n",
    "\n",
    "        third_step = self.gamma * prime_action_selection_prob * self.gamma * double_prime_action_selection_prob * second_step_error\n",
    "\n",
    "        target = first_step + second_step + third_step \n",
    "        self.Q[info[STATE]][info[ACTION]] += ALPHA * (target - self.Q[info[STATE]][info[ACTION]])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 13060,
     "status": "ok",
     "timestamp": 1609760596783,
     "user": {
      "displayName": "Narjes Noorzad",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gj7tkVNhkAPpQLlGPleUdGAeLotodizEcuFCkFIkA=s64",
      "userId": "16921968874120068723"
     },
     "user_tz": -210
    },
    "id": "_-K-Ohvh1inx",
    "outputId": "589ef104-4451-4037-b292-7e95830a4591"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 2000/2000."
     ]
    }
   ],
   "source": [
    "# env =  Environment.GridworldEnv() #<--uncomment for normal environment\n",
    "# env =  Modified_Environment.GridworldEnv() # <--uncomment for modified environment (bonus part)\n",
    "agent = TreeBackupAgent(env, gamma)\n",
    "agent.reach_island()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kcszMJUi2dBC"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(range(TOTAL_EPISODES), agent.TB_episode_scores, color = '#633974')\n",
    "plt.xlabel('episodes ->')\n",
    "plt.ylabel('epsiode score ->')\n",
    "plt.title('Tree Backup(3-step)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cKmdpWC8DHXv"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "window_size = 50\n",
    "smoothed_score = pd.Series(agent.TB_episode_scores).rolling(window_size , min_periods = window_size).mean()\n",
    "plt.plot(smoothed_score, color = '#633974')\n",
    "#plt.annotate(smoothed_score.iloc[-1], xy=(2000, smoothed_score.iloc[-1]), xytext=(2000, smoothed_score.iloc[-1]), color = '#633974',) # <--uncomment for modified environment (bonus part)\n",
    "plt.xlabel(\"epsiode ->\")\n",
    "plt.ylabel(\"epsiode score (smoothed) -> \")\n",
    "plt.title('Tree Backup(3-step)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "executionInfo": {
     "elapsed": 1575,
     "status": "ok",
     "timestamp": 1609760708910,
     "user": {
      "displayName": "Narjes Noorzad",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gj7tkVNhkAPpQLlGPleUdGAeLotodizEcuFCkFIkA=s64",
      "userId": "16921968874120068723"
     },
     "user_tz": -210
    },
    "id": "Qp_oExDuQVFv"
   },
   "outputs": [],
   "source": [
    "#i wrote my reward data into a .txt file to use it later.\n",
    "\n",
    "# with open('3_TB_bonus_scores', 'wb') as fp:\n",
    "#     pickle.dump(agent.TB_episode_scores, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eraMSULtKkEm"
   },
   "source": [
    "second version\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pytj28MDqtHx"
   },
   "outputs": [],
   "source": [
    "# def create_behavior_policy(Q, nA, epsilon=0.3):\n",
    "#     def policy_fn(observations):\n",
    "#         A = np.ones(nA, dtype=np.float) * (epsilon/nA)\n",
    "#         best_action = np.argmax(Q[observations])\n",
    "#         A[best_action] += 1.0 - epsilon\n",
    "#         return A\n",
    "#     return policy_fn\n",
    "\n",
    "# def create_target_policy(Q, nA, epsilon=0.1):\n",
    "#     def policy_fn(observations):\n",
    "#         A = np.ones(nA, dtype=np.float) * (epsilon/nA)\n",
    "#         best_action = np.argmax(Q[observations])\n",
    "#         A[best_action] += 1.0 - epsilon\n",
    "#         return A\n",
    "#     return policy_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tXUoskBTpyEE"
   },
   "outputs": [],
   "source": [
    "# class nStepTreeBackupAgent():\n",
    "#   def __init__(self, env, n, gamma):\n",
    "#       self.env = env\n",
    "#       self.n = n\n",
    "#       self.Q = defaultdict(lambda: np.zeros(env.action_space.n))\n",
    "#       self.actions = env.action_space.n\n",
    "#       self.behavior_policy = create_behavior_policy(self.Q, self.actions)\n",
    "#       self.target_policy = create_target_policy(self.Q, self.actions)\n",
    "#       #self.epsilon = epsilon\n",
    "#       self.gamma = gamma\n",
    "#       self.nstep_TB_episode_scores = []\n",
    "\n",
    "\n",
    "#   def reach_island(self):\n",
    "#     epsilon = 1\n",
    "#     for episode in range(1, TOTAL_EPISODES + 1):\n",
    "#         episode_score = 0\n",
    "#         if episode % 2 == 0:\n",
    "#                 epsilon = max(epsilon * DECAY_FACTOR, MIN)\n",
    "#                 print(\"\\rEpisode {}/{}.\".format(episode, TOTAL_EPISODES), end=\"\")\n",
    "#                 sys.stdout.flush()\n",
    "                \n",
    "#         stored_rewards, stored_states, stored_actions  = {}, {}, {}        \n",
    "#         T, t, tau = sys.maxsize, -1, 0\n",
    "      \n",
    "#         state = env.reset()\n",
    "#         probs = self.behavior_policy(state)\n",
    "#         action = np.random.choice(np.arange(len(probs)), p = probs)\n",
    "\n",
    "#         stored_states[0], stored_actions[0]  = state, action\n",
    "           \n",
    "#         while tau < (T-1):\n",
    "#             t+=1\n",
    "#             if t < T:\n",
    "#                 state, reward, done, _ = env.step(action)\n",
    "                \n",
    "#                 stored_states[(t + 1) % (self.n + 1)], stored_rewards[(t + 1) % (self.n + 1)] = state, reward\n",
    "   \n",
    "#                 episode_score += reward  \n",
    "                              \n",
    "#                 if done or t > MAX_T: T = t+1\n",
    "#                 else:\n",
    "#                     action_probs = self.behavior_policy(state)\n",
    "#                     action = np.random.choice(np.arange(self.actions), p=action_probs)\n",
    "#                     stored_actions[(t + 1) % (self.n + 1)] = action\n",
    "                    \n",
    "#             tau = t - self.n + 1\n",
    "#             information = [DUMMY, stored_states, stored_actions, stored_rewards]\n",
    "#             self.nstep_TB_returns(information, t, tau, T)\n",
    "        \n",
    "#         self.nstep_TB_episode_scores.append(episode_score)\n",
    "\n",
    "\n",
    "#   def nstep_TB_returns(self, information, t, tau, T):\n",
    "#       if tau >= 0:\n",
    "#           if (t + 1) >= T:\n",
    "#               G = information[REWARD][T % (self.n+1)]\n",
    "#           else:\n",
    "#               s_t1 = information[STATE][(t + 1) % (self.n + 1)]\n",
    "#               leaf_sum = np.sum([(self.target_policy(s_t1)[a])*self.Q[s_t1][a] for a in range(self.actions)])\n",
    "#               G = information[REWARD][(t + 1) % (self.n + 1)] + self.gamma * leaf_sum\n",
    "          \n",
    "#           for k in range(min(t, T-1), tau, -1):\n",
    "#               s_k, a_k = information[STATE][k % (self.n + 1)], information[ACTION][k % (self.n + 1)]\n",
    "#               action_probs = np.sum([self.target_policy(s_k)[a]* self.Q[s_k][a] for a in range(self.actions) if a!= a_k])\n",
    "#               G = information[REWARD][k % (self.n + 1)] + self.gamma * (action_probs + self.target_policy(s_k)[a_k]*G)\n",
    "          \n",
    "#           s_tau, a_tau =  information[STATE][tau % (self.n + 1)], information[ACTION][tau % (self.n + 1)]\n",
    "#           self.Q[s_tau][a_tau] += ALPHA * (G - self.Q[s_tau][a_tau])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4153054,
     "status": "ok",
     "timestamp": 1609691620881,
     "user": {
      "displayName": "Narjes Noorzad",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gj7tkVNhkAPpQLlGPleUdGAeLotodizEcuFCkFIkA=s64",
      "userId": "16921968874120068723"
     },
     "user_tz": -210
    },
    "id": "mxcIoOfC3U4U",
    "outputId": "1526c0dd-28ad-44c2-cff1-841b24aa22f5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 2000/2000."
     ]
    }
   ],
   "source": [
    "\n",
    "# agent = nStepTreeBackupAgent(env, n, gamma)\n",
    "# agent.reach_island()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q5gHjv0-oO2s"
   },
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(12, 6))\n",
    "# plt.plot(range(TOTAL_EPISODES), agent.nstep_TB_episode_scores, color = 'thistle', label = '$\\epsilon$ = from $1$ to $0.1$')\n",
    "# plt.xlabel('episodes ->')\n",
    "# plt.ylabel('epsiode score ->')\n",
    "# plt.title('Tree Backup(3-step)')\n",
    "# plt.legend()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kXpAQRSUekYd"
   },
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(12, 6))\n",
    "# window_size = 50\n",
    "# smoothed_score = pd.Series(agent.nstep_TB_episode_scores).rolling(window_size , min_periods = window_size).mean()\n",
    "# plt.plot(smoothed_score, color = 'thistle',label = '$\\epsilon$ = from $1$ to $0.1$')\n",
    "# plt.xlabel(\"epsiode ->\")\n",
    "# plt.ylabel(\"epsiode score (smoothed) -> \")\n",
    "# plt.title('Tree Backup(3-step)')\n",
    "# plt.legend()\n",
    "# plt.show()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMtr04MaH+GnQz/7k3zAFEO",
   "collapsed_sections": [],
   "name": "treebackup.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
