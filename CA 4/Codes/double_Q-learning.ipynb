{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"double_Q-learning.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyN+yZxLIjDHTMS9ieOWvoA9"},"kernelspec":{"display_name":"Python 3","name":"python3"}},"cells":[{"cell_type":"code","metadata":{"id":"tQLpsc19eSqr"},"source":["from google.colab import drive\r\n","drive.mount('/content/drive')\r\n","!pip install import-ipynb\r\n","import import_ipynb"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wd5LVe8ReYSC"},"source":["%cd drive\r\n","%cd 'My Drive'\r\n","%cd 'RL'\r\n","%cd 'Homework'\r\n","%cd 'Homework 4'\r\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rlzOiUKHeadd","executionInfo":{"status":"ok","timestamp":1609761208351,"user_tz":-210,"elapsed":1444,"user":{"displayName":"Narjes Noorzad","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj7tkVNhkAPpQLlGPleUdGAeLotodizEcuFCkFIkA=s64","userId":"16921968874120068723"}}},"source":["'''Needed Libraries and Environment'''\r\n","import Environment\r\n","import Modified_Environment\r\n","import matplotlib.pyplot as plt\r\n","import numpy as np\r\n","import math\r\n","import random\r\n","from collections import defaultdict\r\n","import sys\r\n","import itertools\r\n","import matplotlib\r\n","import pandas as pd\r\n","import pickle\r\n","\r\n","'''Constants Defined'''\r\n","NEXTSTATE = 0\r\n","STATE = 1\r\n","ACTION = 2\r\n","REWARD = 3\r\n","MAX_T = 1000\r\n","TOTAL_EPISODES = 2000\r\n","gamma = 0.9\r\n","DECAY_FACTOR = 0.99\r\n","MIN = 0.001\r\n","ALPHA = 0.2"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"id":"StR7y4zseEJA","executionInfo":{"status":"ok","timestamp":1609761209432,"user_tz":-210,"elapsed":1242,"user":{"displayName":"Narjes Noorzad","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj7tkVNhkAPpQLlGPleUdGAeLotodizEcuFCkFIkA=s64","userId":"16921968874120068723"}}},"source":["class DoubleQlearningAgent():\r\n","  \r\n","    def __init__(self, env, gamma):\r\n","      self.env = env\r\n","      self.Q = defaultdict(lambda: np.zeros(env.action_space.n))\r\n","      self.Q_A = defaultdict(lambda: np.zeros(env.action_space.n))\r\n","      self.Q_B = defaultdict(lambda: np.zeros(env.action_space.n))\r\n","      self.actions = env.action_space.n\r\n","      self.gamma = gamma\r\n","      self.DQ_episode_scores = []\r\n","\r\n","\r\n","    def epsilon_greedy_policy(self, state, epsilon):\r\n","        epsilon_policy = np.ones(self.actions, dtype=float) * epsilon / self.actions\r\n","        best_action = np.argmax(self.Q_A[state] + self.Q_B[state])\r\n","        epsilon_policy[best_action] += (1.0 - epsilon)\r\n","        return epsilon_policy\r\n","\r\n","\r\n","    def reach_island(self):\r\n","        #epsilon = 1 #<-- uncomment if you want to use decaying epsilon \r\n","        #epsilon = 0.3 #<-- uncomment if you want to use constant epsilon\r\n","        for episode in range(1, TOTAL_EPISODES + 1):\r\n","            episode_score = 0\r\n","            if episode % 2 == 0:\r\n","                print(\"\\rEpisode {}/{}.\".format(episode, TOTAL_EPISODES), end=\"\")\r\n","                #epsilon = max(epsilon * DECAY_FACTOR, MIN)#<-- uncomment if you want to use decaying epsilon\r\n","                sys.stdout.flush()\r\n","            state = self.env.reset()\r\n","            done = False\r\n","            for t in itertools.count():\r\n","                probs = self.epsilon_greedy_policy(state, epsilon)\r\n","                action = np.random.choice(np.arange(len(probs)), p = probs)\r\n","                next_state, reward, done, _ = self.env.step(action)\r\n","                episode_score += reward\r\n","                chosen_number = random.randint(1,2)\r\n","                information = [next_state, state, action, reward]\r\n","                self.update_chosen_Q(information, chosen_number)\r\n","                if done or t > MAX_T :break\r\n","                self.Q[state][action] = (self.Q_A[state][action] + self.Q_B[state][action])\r\n","                state = next_state\r\n","                sys.stdout.flush()\r\n","            \r\n","            self.DQ_episode_scores.append(episode_score)\r\n","\r\n","    def update_chosen_Q(self, info, chosen_number):\r\n","        if chosen_number == 1 :\r\n","            self.Q_A[info[STATE]][info[ACTION]] += ALPHA * (info[REWARD] + self.gamma * self.Q_B[info[NEXTSTATE]][np.argmax(self.Q_A[info[NEXTSTATE]][:])] - self.Q_A[info[STATE]][info[ACTION]])\r\n","        elif chosen_number == 2:\r\n","            self.Q_B[info[STATE]][info[ACTION]] += ALPHA * (info[REWARD] + self.gamma * self.Q_A[info[NEXTSTATE]][np.argmax(self.Q_B[info[NEXTSTATE]][:])] - self.Q_B[info[STATE]][info[ACTION]])\r\n","\r\n"],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5BsuJqahpxrY","executionInfo":{"status":"ok","timestamp":1609761254582,"user_tz":-210,"elapsed":30127,"user":{"displayName":"Narjes Noorzad","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj7tkVNhkAPpQLlGPleUdGAeLotodizEcuFCkFIkA=s64","userId":"16921968874120068723"}},"outputId":"3aabebdc-3df2-4e2c-ba84-7e3e8e7fd1ca"},"source":["# env =  Environment.GridworldEnv() #<--uncomment for normal environment\r\n","# env =  Modified_Environment.GridworldEnv() # <--uncomment for modified environment (bonus part)\r\n","agent = DoubleQlearningAgent(env, gamma)\r\n","agent.reach_island()"],"execution_count":8,"outputs":[{"output_type":"stream","text":["Episode 2000/2000."],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"4Y6n7W6tpH0i"},"source":["plt.figure(figsize=(12, 6))\r\n","plt.plot(range(TOTAL_EPISODES), agent.DQ_episode_scores, color = '#633974', label = '$\\epsilon$ = from $1$ to $0.001$')\r\n","plt.xlabel('episodes ->')\r\n","plt.ylabel('epsiode score ->')\r\n","plt.title('Double Q-learning')\r\n","plt.legend()\r\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"n3Ny8ojjpvv-"},"source":["plt.figure(figsize=(12, 6))\r\n","window_size = 50\r\n","smoothed_score = pd.Series(agent.DQ_episode_scores).rolling(window_size , min_periods=window_size).mean()\r\n","plt.plot(smoothed_score, color = '#633974', label = '$\\epsilon$ = from $1$ to $0.001$')\r\n","#plt.annotate(smoothed_score.iloc[-1], xy=(2000, smoothed_score.iloc[-1]), xytext=(2000, smoothed_score.iloc[-1]), color = '#633974',) # <--uncomment for modified environment (bonus part)\r\n","plt.xlabel(\"epsiode ->\")\r\n","plt.ylabel(\"epsiode score (smoothed) -> \")\r\n","plt.title('Double Q-learning')\r\n","plt.legend()\r\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"61YzdCUvvfZ9","executionInfo":{"status":"ok","timestamp":1609761267304,"user_tz":-210,"elapsed":1078,"user":{"displayName":"Narjes Noorzad","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj7tkVNhkAPpQLlGPleUdGAeLotodizEcuFCkFIkA=s64","userId":"16921968874120068723"}}},"source":["#i wrote my reward data into a .txt file to use it later.\r\n","\r\n","# with open('DQ_bonus_scores', 'wb') as fp:\r\n","#     pickle.dump(agent.DQ_episode_scores, fp)"],"execution_count":11,"outputs":[]}]}