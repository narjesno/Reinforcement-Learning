{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"n-step_Expected_SARSA.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyML7xknqL3dPF9mEHZsxZK3"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"I4PIL8794kAl"},"source":["from google.colab import drive\r\n","drive.mount('/content/drive')\r\n","!pip install import-ipynb\r\n","import import_ipynb"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8rKFnmy04zer"},"source":["%cd drive\r\n","%cd 'My Drive'\r\n","%cd 'RL'\r\n","%cd 'Homework'\r\n","%cd 'Homework 4'\r\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YddiligIMOn3","executionInfo":{"status":"ok","timestamp":1609762999762,"user_tz":-210,"elapsed":1200,"user":{"displayName":"Narjes Noorzad","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj7tkVNhkAPpQLlGPleUdGAeLotodizEcuFCkFIkA=s64","userId":"16921968874120068723"}}},"source":["'''Needed Libraries and Environment'''\r\n","import Environment\r\n","import Modified_Environment\r\n","import matplotlib.pyplot as plt\r\n","import numpy as np\r\n","import math\r\n","import random\r\n","from collections import defaultdict\r\n","import sys\r\n","import itertools\r\n","import matplotlib\r\n","import pandas as pd\r\n","import pickle\r\n","\r\n","'''Constants Defined'''\r\n","STATE = 0\r\n","ACTION = 1\r\n","REWARD = 2\r\n","MAX_T = 10000\r\n","TOTAL_EPISODES = 2000\r\n","gamma = 0.9\r\n","ALPHA = 0.2\r\n","MIN = 0.1\r\n","DECAY_FACTOR = 0.99"],"execution_count":19,"outputs":[]},{"cell_type":"code","metadata":{"id":"ol3Z18YONr8d","executionInfo":{"status":"ok","timestamp":1609763638543,"user_tz":-210,"elapsed":1269,"user":{"displayName":"Narjes Noorzad","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj7tkVNhkAPpQLlGPleUdGAeLotodizEcuFCkFIkA=s64","userId":"16921968874120068723"}}},"source":["class nStepExpectedSarsa():\r\n","  def __init__(self, env, gamma, n = 2):\r\n","      self.env = env\r\n","      self.gamma = gamma\r\n","      self.n = n\r\n","      self.actions = env.action_space.n\r\n","      self.nstep_expsarsa_episode_scores = []\r\n","      self.Q = defaultdict(lambda: np.zeros(env.action_space.n))\r\n","\r\n","  def epsilon_greedy_policy(self, state, epsilon):\r\n","      epsilon_policy = np.ones(self.actions, dtype=float) * epsilon / self.actions\r\n","      best_action = np.argmax(self.Q[state])\r\n","      epsilon_policy[best_action] += (1.0 - epsilon)\r\n","      return epsilon_policy\r\n","\r\n","  def reach_island(self):\r\n","      #epsilon = 1 #<-- uncomment if you want to use decaying epsilon\r\n","      #epsilon = 0.5 #<-- uncomment if you want to use decaying epsilon (bonus part)\r\n","      #epsilon = 0.3 #<-- uncomment if you want to use constant epsilon\r\n","      for episode in range(1, TOTAL_EPISODES + 1):\r\n","          episode_score = 0\r\n","          if episode % 2 == 0:\r\n","                print(\"\\rEpisode {}/{}.\".format(episode, TOTAL_EPISODES), end=\"\")\r\n","                sys.stdout.flush()\r\n","                #epsilon = max(epsilon * DECAY_FACTOR, MIN)  #<-- uncomment if you want to use decaying epsilon\r\n","          stored_rewards, stored_states, stored_actions  = {}, {}, {}        \r\n","          T, t, tau = sys.maxsize, -1, 0\r\n","          state = env.reset()\r\n","          probs = self.epsilon_greedy_policy(state, epsilon)\r\n","          action = np.random.choice(np.arange(len(probs)), p = probs)\r\n","\r\n","          stored_states[0], stored_actions[0]  = state, action\r\n","          \r\n","          while tau < (T - 1):\r\n","              t += 1\r\n","              if t < T:\r\n","                  next_state, reward, done, _ = env.step(action)\r\n","                  state = next_state\r\n","                  stored_states[(t + 1) % (self.n + 1)],  stored_rewards[(t + 1) % (self.n + 1)] = state, reward \r\n","                  episode_score += reward   \r\n","                  \r\n","                  if done or t > MAX_T: T = t + 1\r\n","                  else:\r\n","                      probs = self.epsilon_greedy_policy(state, epsilon)\r\n","                      action = np.random.choice(np.arange(len(probs)), p = probs)\r\n","                      stored_actions[(t + 1) % (self.n + 1)] = action\r\n","              \r\n","              tau = t - self.n + 1\r\n","              information = [stored_states, stored_actions, stored_rewards]\r\n","              self.expected_sarsa_returns(information, tau, T, epsilon)\r\n","          self.nstep_expsarsa_episode_scores.append(episode_score)\r\n","\r\n","  def expected_sarsa_returns(self, info, tau, T, epsilon):\r\n","      if tau >= 0:\r\n","          G = np.sum([(self.gamma**(i-tau-1)) * info[REWARD][i % (self.n + 1)] for i  in range(tau + 1, min(tau + self.n, T) + 1)])\r\n","          \r\n","          if tau + self.n < T:\r\n","              exp_sarsa_update = np.sum([self.epsilon_greedy_policy(info[STATE][(tau + self.n) % (self.n + 1)], epsilon)[a] * self.Q[info[STATE][(tau + self.n) % (self.n + 1)]][a] for a in range(self.actions)])\r\n","              G += (self.gamma ** self.n) * exp_sarsa_update\r\n","          \r\n","          s_tau, a_tau = info[STATE][tau % (self.n + 1)], info[ACTION][tau % (self.n + 1)]\r\n","          self.Q[s_tau][a_tau] += ALPHA * (G - self.Q[s_tau][a_tau])\r\n","  "],"execution_count":22,"outputs":[]},{"cell_type":"code","metadata":{"id":"bEmjaxdoZlQM","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1609763712398,"user_tz":-210,"elapsed":11205,"user":{"displayName":"Narjes Noorzad","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj7tkVNhkAPpQLlGPleUdGAeLotodizEcuFCkFIkA=s64","userId":"16921968874120068723"}},"outputId":"246032fb-bdc0-4f83-c3c8-cec5cc669664"},"source":["#env =  Environment.GridworldEnv() #<--uncomment for normal environment\r\n","#env =  Modified_Environment.GridworldEnv() # <--uncomment for modified environment (bonus part)\r\n","agent = nStepExpectedSarsa(env, gamma)\r\n","agent.reach_island()"],"execution_count":31,"outputs":[{"output_type":"stream","text":["Episode 2000/2000."],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"kYA2m6A184xf"},"source":["plt.figure(figsize=(12, 6))\r\n","plt.plot(range(TOTAL_EPISODES), agent.nstep_expsarsa_episode_scores, color = '#633974',label = '$\\epsilon$ = from $0.5$ to $0.1$')\r\n","plt.xlabel('episodes ->')\r\n","plt.ylabel('epsiode score ->')\r\n","plt.title('2-Step Expected SARSA')\r\n","plt.legend()\r\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Um1zS5rL84kR"},"source":["plt.figure(figsize=(12, 6))\r\n","window_size = 50\r\n","smoothed_score = pd.Series(agent.nstep_expsarsa_episode_scores).rolling(window_size, min_periods=window_size).mean()\r\n","plt.plot(smoothed_score, color = '#633974', label = '$\\epsilon$ = from $0.5$ to $0.1$')\r\n","plt.annotate(smoothed_score.iloc[-1], xy=(2000, smoothed_score.iloc[-1]), xytext=(2000, smoothed_score.iloc[-1]), color = '#633974',) # <--uncomment for modified environment (bonus part)\r\n","plt.xlabel(\"epsiode ->\")\r\n","plt.ylabel(\"epsiode score (smoothed) -> \")\r\n","plt.title('2-Step Expected SARSA')\r\n","plt.legend()\r\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZmSHrTs1bcpu","executionInfo":{"status":"ok","timestamp":1609764023198,"user_tz":-210,"elapsed":1185,"user":{"displayName":"Narjes Noorzad","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj7tkVNhkAPpQLlGPleUdGAeLotodizEcuFCkFIkA=s64","userId":"16921968874120068723"}}},"source":["#i wrote my reward data into a .txt file to use it later.\r\n","\r\n","# with open('nstep_expsarsa_bonus_scores', 'wb') as fp:\r\n","#     pickle.dump(agent.nstep_expsarsa_episode_scores, fp)"],"execution_count":34,"outputs":[]}]}