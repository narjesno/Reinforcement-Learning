{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HW3_Q1.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "pe-3mWTfic-8"
      },
      "source": [
        "import numpy as np\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "import gym\n",
        "from amalearn.environment import EnvironmentBase\n",
        "A = 0\n",
        "B = 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "### Parameters given in the question"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H-upiRAbioBC"
      },
      "source": [
        "class Parameters:\n",
        "    def __init__(self):\n",
        "        self.max_capacity = 20\n",
        "        self.max_purchase = 5\n",
        "        self.data_cost = 10\n",
        "        self.purchase_cost = 2 #for part b, change it to 6\n",
        "        self.theta = 0.01\n",
        "        self.gamma = 0.9 #for part a.2, change to 1 \n",
        "        self.lambda_request_A = 3\n",
        "        self.lambda_request_B = 4\n",
        "        self.lambda_terminate_A = 3\n",
        "        self.lambda_terminate_B = 2\n",
        "        self.upper_bound = 3"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "### Poisson Distribution for caclulating probability of a going to a state"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WA7_qJ56ipD6"
      },
      "source": [
        "def poisson(lambdaa, occurrences_count) : return (lambdaa**occurrences_count) * np.exp(-lambdaa) / math.factorial(occurrences_count)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "### The main class for solving the problem using PI"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eWr5rIs6irFm"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "class PolicyIteration(object):\n",
        "    def __init__(self, params, env):\n",
        "      self.env = env\n",
        "      self.params = params\n",
        "      self.states = [(x, y) for x in range(self.params.max_capacity + 1) for y in range(self.params.max_capacity + 1)]\n",
        "      self.policy = np.zeros([self.params.max_capacity + 1]*2, int)\n",
        "      self.policies = []\n",
        "\n",
        "    def policy_eval(self, policy):\n",
        "      while True:\n",
        "        delta = 0\n",
        "        for state in self.states:\n",
        "            v = env.state_value[state]\n",
        "            env.set_current_state(state)\n",
        "            env.state_value[state] = env.calculate_reward(self.policy[state])\n",
        "            delta = max(delta, abs(env.state_value[state] - v))\n",
        "        if delta < self.params.theta: break\n",
        "        print(\"Delta is:\", delta)\n",
        "\n",
        "\n",
        "    def solve_problem(self):\n",
        "      iter = 0\n",
        "      while True:\n",
        "        print(\"iteration #\",iter)\n",
        "        iter +=1\n",
        "        self.policies.append(self.policy.copy())\n",
        "        self.policy_eval(self.policy)\n",
        "        policy_stable = True\n",
        "        for state in self.states:\n",
        "            old_action = self.policy[state]\n",
        "            env.set_current_state(state)\n",
        "            values_dict = {action: env.calculate_reward(action) for action in env.available_actions()}\n",
        "            self.policy[state] = np.random.choice([action for action, value in values_dict.items() if value == max(list(values_dict.values()))])\n",
        "            if old_action != self.policy[state]: policy_stable = False\n",
        "        if policy_stable: break\n",
        "\n",
        "    def print_policies(self):\n",
        "        for idx, policy in enumerate(self.policies):\n",
        "          plt.figure()\n",
        "          plt.imshow(policy, origin='lower', interpolation='none', vmin=-self.params.max_purchase, vmax=self.params.max_purchase, cmap = 'Purples')\n",
        "          plt.xlabel('#Data at second company')\n",
        "          plt.ylabel('#Data at first company')\n",
        "          plt.title('policy{:d}'.format(idx))\n",
        "          plt.colorbar()\n",
        "\n",
        "    def print_value(self):\n",
        "        fig = plt.figure()\n",
        "        fig.set_size_inches(8, 5)\n",
        "        ax = fig.gca(projection='3d')\n",
        "        X = np.arange(0, self.params.max_capacity + 1)\n",
        "        Y = np.arange(0, self.params.max_capacity + 1)\n",
        "        X, Y = np.meshgrid(X, Y)\n",
        "        ax.plot_surface(X, Y, env.state_value, cmap = 'Purples')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "### Environment for our agent "
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class MDPEnvironment_(EnvironmentBase):\n",
        "    def __init__(self, params):\n",
        "\n",
        "        self.current_state = None\n",
        "        self.params = params\n",
        "        n_states = (self.params.max_capacity + 1)**2\n",
        "        n_actions = 11\n",
        "        self.state_value = np.zeros([self.params.max_capacity + 1]*2)\n",
        "        state_space = gym.spaces.Discrete(n_states)\n",
        "        action_space = gym.spaces.Discrete(n_actions)\n",
        "        self.reset()\n",
        "        super().__init__(action_space, state_space, id, None)\n",
        "\n",
        "    def next_state(self, action):\n",
        "        self.episode_length += 1\n",
        "        ns = np.random.choice(list(range(self.observation_space.n)), p=self.P_s_s_a[self.s_current, :, action])\n",
        "        return ns\n",
        "\n",
        "    def available_actions(self):\n",
        "        return np.arange(self.action_space.n, dtype='int32')\n",
        "\n",
        "    def terminated(self): return self.episode_length > 20\n",
        "\n",
        "    def observe(self): return self.s_current\n",
        "\n",
        "    def get_info(self, action): return {}\n",
        "\n",
        "    def reset(self):\n",
        "        self.s_current = 0\n",
        "        self.s_previous = None\n",
        "        self.episode_length = 0\n",
        "\n",
        "    def close(self): return\n",
        "\n",
        "    def render(self, mode='human'): return\n",
        "\n",
        "    def set_current_state(self, state): self.current_state = state\n",
        "\n",
        "    def available_actions(self):\n",
        "        actions = []\n",
        "        given_state = self.current_state\n",
        "        A_state, B_state = given_state\n",
        "        all_actions = [act for act in range(-self.params.max_purchase, self.params.max_purchase + 1)]\n",
        "        for action in all_actions:\n",
        "            if A_state - action < 0 or A_state - action > self.params.max_capacity: continue \n",
        "            if B_state + action < 0 or B_state + action > self.params.max_capacity: continue\n",
        "            actions.append(action)\n",
        "        print(\"Available actions:\", actions, \"State:\",given_state )\n",
        "        return actions\n",
        "\n",
        "\n",
        "    def calculate_reward(self, action):\n",
        "        states = self.current_state\n",
        "        returns = 0\n",
        "        returns -= self.params.purchase_cost * abs(action)\n",
        "        data_A = int(min(states[A] - action, self.params.max_capacity))\n",
        "        data_B = int(min(states[B] + action, self.params.max_capacity))\n",
        "        for request_A in range(self.params.upper_bound):\n",
        "            for request_B in range(self.params.upper_bound):\n",
        "                requests_probability = poisson(self.params.lambda_request_A, request_A) * poisson(self.params.lambda_request_B,request_B)\n",
        "                total_requestA, total_requestB = min(data_A, request_A), min(data_B, request_B)\n",
        "                rewards = (total_requestA + total_requestB) * self.params.data_cost\n",
        "                for terminate_A in range(self.params.upper_bound):\n",
        "                    for terminate_B in range(self.params.upper_bound):\n",
        "                        returns_probability = poisson(self.params.lambda_terminate_A, terminate_A) * poisson(self.params.lambda_terminate_B, terminate_B)\n",
        "                        left_data_A = min(data_A - total_requestA + terminate_A, self.params.max_capacity)\n",
        "                        left_data_B = min(data_B - total_requestB + terminate_B, self.params.max_capacity)\n",
        "                        returns += returns_probability * requests_probability * (rewards + self.params.gamma * self.state_value[left_data_A, left_data_B])\n",
        "        return returns\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dni1no2Zixbi",
        "outputId": "ecbee61b-a637-4727-f565-38076ba9af99",
        "tags": [
          "outputPrepend"
        ]
      },
      "source": [
        "\n",
        "params = Parameters()\n",
        "env = MDPEnvironment_(params)\n",
        "policy_iteration = PolicyIteration(params, env)\n",
        "policy_iteration.solve_problem()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "rEB420cTZml4",
        "outputId": "23289c58-a166-4e20-ac81-0d1f2396fbe3"
      },
      "source": [
        "policy_iteration.print_policies()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 247
        },
        "id": "eTqBZl-pZp9F",
        "outputId": "100c832c-d7e9-4b61-8c39-849854f7633b"
      },
      "source": [
        "policy_iteration.print_value()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}